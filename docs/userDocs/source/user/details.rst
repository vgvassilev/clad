Advance details
======================================

This section describes about advance details and features of Clad. It also gives
insights about the inner working of Clad.

Clad usage
-----------

Clad provides four API functions:

- ``clad::differentiate`` to use forward-mode AD
- ``clad::gradient`` to use reverse-mode AD
- ``clad::hessian`` to compute Hessian matrix using a combination of forward-mode and reverse-mode AD
- ``clad::jacobian`` to compute Jacobian matrix using reverse-mode AD

API functions are used to label an existing function for differentiation.
Both functions return a functor object containing the generated derivative
which can be called via ``.execute`` method, which forwards provided arguments
to the generated derivative function. Example::

  #include "clad/Differentiator/Differentiator.h"
  #include <iostream>

  double f(double x, double y) { return x * y; }

  int main() {
    auto f_dx = clad::differentiate(f, "x");
    // Computes derivative of 'f' when (x, y) = (3, 4)
    std::cout << f_dx.execute(3, 4) << std::endl; // prints: 4
    f_dx.dump(); // prints:
    /* double f_darg0(double x, double y) {
         double _d_x = 1; double _d_y = 0;
         return _d_x * y + x * _d_y;
       } */
  }

.. _forward_mode:

Forward mode
---------------

For a function ``f`` of several inputs and single (scalar) output, forward 
mode AD can be used to compute (or, in case of Clad, create a function) 
computing a directional derivative of ``f`` with respect to a *single* 
specified input variable. The derivative function created by the forward-mode 
AD is guaranteed to have *at most* a constant factor (around 2-3) more 
arithmetical operations compared to the original function.

``clad::differentiate(f, ARGS)`` takes 2 arguments:

1. ``f`` is a pointer to a function or a method to be differentiated
2. ``ARGS`` is either:

  - a single numerical literal indicating an index of independent variable (e.g. ``0`` for ``x``, ``1`` for ``y``)
  - a string literal with the name of independent variable (as stated in the *definition* of ``f``, e.g. ``"x"`` or ``"y"``); if the variable is an array the index needs to be specified, e.g. `"arr[1]"``

The generated derivative function has the same signature as the original function ``f``, however its return value is the value of the derivative.

.. _reverse_mode:

Reverse mode
----------------

When a function has many inputs and derivatives w.r.t. every input (i.e. gradient vector) are required, reverse-mode AD is a better alternative to the forward-mode. Reverse-mode AD allows to compute the gradient of ``f`` using *at most* a constant factor (around 4) more arithmetical operations compared to the original function. While its constant factor and memory overhead is higher than that of the forward-mode, it is independent of the number of inputs. E.g. for a function having N inputs and consisting of T arithmetical operations, computing its gradient takes a single execution of the reverse-mode AD and around 4\*T operations, while it would take N executions of the forward-mode, this requiring up to N\*3\*T operations.

``clad::gradient(f, /*optional*/ ARGS)`` takes 1 or 2 arguments:

1. ``f`` is a pointer to a function or a method to be differentiated
2. ``ARGS`` is either:

  - not provided, then ``f`` is differentiated w.r.t. its every argument
  - a string literal with comma-separated names of independent variables (e.g. ``"x"`` or ``"y"`` or ``"x, y"`` or ``"y, x"``)

Since a vector of derivatives must be returned from a function generated by the
reverse mode, its signature is slightly different. The generated function has 
``void`` return type and the same input arguments. The function has additional, last
input argument of the type ``T*``, where ``T`` is the return type of ``f``. This
is the "result" argument which has to point to the beginning of the vector where
the gradient will be stored. *The caller is responsible for allocating and zeroing-out the gradient storage*. Example::


  auto f_grad = clad::gradient(f);
  double result1[2] = {};
  clad::array_ref<double> result1_ref(result1, 2);
  f_grad.execute(x, y, result1_ref);
  std::cout << "dx: " << result1[0] << ' ' << "dy: " << result1[1] << std::endl;

  auto f_dx_dy = clad::gradient(f, "x, y"); // same effect as before

  auto f_dy_dx = clad::gradient(f, "y, x");
  double result2[2] = {};
  clad::array_ref<double> result2_ref(result2, 2);
  f_dy_dx.execute(x, y, result2_ref);
  // note that the derivatives are mapped to the "result" indices in the same order as they were specified in the argument:
  std::cout << "dy: " << result2[0] << ' ' << "dx: " << result2[1] << std::endl;

.. note::
   *we are working on improving the gradient interface*.

What can be differentiated
----------------------------

Clad is based on compile-time analysis and transformation of C++ abstract syntax tree (Clang AST). This means that Clad must be able to see the body of a function to differentiate it (e.g. if a function is defined in an external library there is no way for Clad to get its AST).

We aim to support every piece of modern C++ syntax, however at the moment only the a 
subset of C++ is supported and there are some constraints on functions that can be 
differentiated with Clad:

- Builtin C++ scalar numeric types (e.g. ``double``, ``float``, ``int``)
- Functions with single return value of supported scalar numeric type.
- Functions with an arbitrary number inputs of supported scalar numeric types.
- ``struct``\ /\ ``class`` methods are supported, however at the moment they
  can only be differentiated w.r.t. member fields using the forward mode AD.

.. note::
   *we are currently working on vector inputs*

The following subset of C++ syntax is supported at the moment:
- Numerical literals, builtin arithmetic operators `+`, `-`, `*`, `/`
- Variable declarations of supported types (including local variables in `{}` blocks)
- Inside functions, builtin arrays (e.g. `double x[1][2][3];`) of supported types and subscript operator `x[i]`
- Direct assignments to variables via `=` and `+=`, `-=`, `*=`, `/=`, `++`, `--`
- Conditional operator `?:` and boolean expressions
- Comma operator `,`
- Control flow: `if` statements and `for` loops (*work on loops in the reverse-mode is in progress*)
- Calls to other functions, including recursion

Specifying custom derivatives
--------------------------------

At times Clad may be unable to differentiate your function (e.g. if its definition is 
in a library and the source code is not available) or an efficient/more numerically 
stable expression for derivatives may be known. In such cases, it is useful 
to be able to specify custom derivatives for your function.

Clad supports that functionality by allowing the user to specify their own derivatives
in ``namespace custom_derivatives``. For a function named ``FNAME`` oen can specify:

- a custom derivative w.r.t ``I``-th argument by defining a function 
  ``FNAME_dargI`` inside ``namespace custom_derivatives``.
- a custom gradient w.r.t every argument by defining a function 
  ``FNAME_grad`` inside ``namespace custom_derivatives``.

When Clad will encounter a function ``FNAME``, it will firstly search for a 
suitable custom function definition within the custom_derivatives namespace. 
Provided no definition was found, Clad will proceed to derive 
the function.

Example:

- Suppose that you have a function ``my_pow(x, y)`` which computes ``x`` to 
  the power of ``y``. In this example, Clad is not able to differentiate ``my_pow``'s 
  body 
  (e.g. it calls an external library or uses some non-differentiable approximation)::

    double my_pow(double x, double y) { // something non-differentiable here... }

However, the analytical formulas of its derivatives are known, thus one can easily 
specify custom derivatives::

  namespace custom_derivatives {
    double my_pow_darg0(double x, double y) { return y * my_pow(x, y - 1); }
    double my_pow_darg1(dobule x, double y) { return my_pow(x, y) * std::log(x); }
  }

Moreover, a custom gradient can be specified::

  namespace custom_derivatives {
    void my_pow_grad(double x, double y, double* result) {
       double t = my_pow(x, y - 1);
       result[0] = y * t;
       result[1] = x * t * std::log(x);
     }
  }

Whenever Clad will encounter ``my_pow`` inside a differentiated function, it 
will find and use the provided custom funtions instead of attempting to 
differentiate it.

.. note::
   Clad provides custom derivatives for some mathematical functions from 

``<cmath>`` inside ``clad/Differentiator/BuiltinDerivatives.h``.

.. note::
   *the concept of custom_derivatives will be reviewed soon, we intend to provide a different interface and avoid function name-based specifications and by-name lookups*.

How Clad works
------------------

Clad is a plugin for the Clang compiler. It relies on the Clang to build the 
AST (`Clang AST <https://clang.llvm.org/docs/IntroductionToTheClangAST.html>`_) 
of user's source code. Then, 
`CladPlugin <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.h#L48>`_\ , 
implemented as `clang::ASTConsumer` analyzes the AST to find differentiation 
requests for clad and process those requests by building Clang AST for 
derivative functions. The whole clad's operation sequence is the following:

- Clang parses user's source code and builds the AST.

- `CladPlugin <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L63>`_ 
  analyzes the built AST and starts the traversal via 
  `HandleTopLevelDecl <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L67>`_.

- `DiffCollector <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/lib/Differentiator/DiffPlanner.cpp#L141>`_
   does the `traversal <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L79>`_
   and looks at every call expression to find all the differentiation 
   requests (through calls to ``clad::differentiate(f, ...)`` or
   ``clad::gradient(f, ...)``).

- ``CladPlugin`` 
  `processes <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L82>`_ every found request.

- Processing each request involves calling to 
  `DerivativeBuilder::Derive <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L113>`_
  which then 
  `switches <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/lib/Differentiator/DerivativeBuilder.cpp#L77>`_
  do either 
  `ForwardModeVisitor::Derive <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/lib/Differentiator/DerivativeBuilder.cpp#L407>`_ 
  or `ReverseModeVisitor::Derive <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/lib/Differentiator/DerivativeBuilder.cpp#L1506>`_, 
  depending on which AD mode was requested.

- ``ForwardModeVisitor`` and ``ReverseModeVisitor`` are derived from 
  ``clang::StmtVisitor``. In the Derive method, clad forward and reverse 
  mode visitors analyze the AST of the declaration for the original 
  function and create the AST for the declaration of the derivative 
  function. Next, they recursively Visit every Stmt in the 
  original function's body and build the body for the 
  derivative function. Forward/Reverse mode AD algorithm is implemented in `Visit...` methods, 
  which are executed depending on the kind of AST node visited.
- The AST of the newly built derivative function's declaration is returned to 
  ``CladPlugin``, where the call to ``clad::differentiate(f, ...)/clad::gradient(f, ...)`` 
  is `updated <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L122>`_ 
  and ``f`` is replaced by a reference to the newly created derivative function 
  ``f_...``. This effectively results in the execution of 
  ``clad::differentiate(f_...)/clad::gradient(f_...)``, which constructs 
  ``CladFunction`` with a pointer to the newly created derivative. 
  Therefore, succeeding user calls to ``.execute`` method will invoke the newly 
  generated derivative.

- Finally, the derivative's AST is 
  `passed <https://github.com/vgvassilev/clad/blob/a264195f00792feeebe63ac7a8ab815c02d20eee/tools/ClangPlugin.cpp#L145>`_ 
  for further processing by Clang compiler (LLVM IR generation, optimizations, machine code generation, etc.).

.. _clad_limitations:

Clad limitations
------------------

.. todo::
  Add clad limitations, some of the limitations are:
  
  1) Do not support differentiating overloaded functions.
  2) Clad differentiation functions do not work in non top-level contexts, 
     such as inside a function.
  3) Many C++ syntax are not supported such as complex pointer usage,
     for-range loops, new and delete operators, iterators usage, custom classes etc
  4) Currently ``clad::jacobian`` do not support array differentiation.
  5) Error estimation framework do not support forward propagation of errors.

.. note::

   Clad currently differentiates types such as `int`/`char`/`boolean` as any real type (such as `float`, `double`, etc.) would be differentiated. Users should keep in mind that Clad *does not* warn against lossy casts, which on differentiation may result in incorrect derivatives.
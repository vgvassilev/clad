#include <chrono>
#include <iostream>
#include <random>
#include "clad/Differentiator/Differentiator.h"

/*
  The code below implements a simple neural network with 3 hidden nodes and 1
  output node. The neural network is trained using gradient descent to minimize
  the mean squared error loss. The code uses Clad to automatically generate the
  gradient computation code for the loss function.

  To compile the code, use the following command:
  clang++ -std=c++14 -I<path-to-clad-include-dir> -fplugin=<path-to-clad.so>
  basic-nn.cpp -DBENCHMARK_MATRIX_MULTIPLY
*/

#define M 100       // num of data points
#define N 100       // num of features
#define N_ITER 5000 // num of iterations

void relu(double* arr, unsigned int n) {
  for (int i = 0; i < n; ++i)
    arr[i] = (arr[i] > 0) ? arr[i] : 0;
}

// Create a model of a neural network with 3 hidden nodes and 1 output node.
// It takes in input data x and weight vectors for each hidden node and the
// output node.
double model_fn(double x[N],  // input data
                double w1[N], // weight vector for hidden node 1
                double w2[N], // weight vector for hidden node 2
                double w3[N], // weight vector for hidden node 3
                double w4[3]  // weight vector for output node
) {
  // Layer 1 - matrix multiplication
  double h1 = 0, h2 = 0, h3 = 0;
  for (int i = 0; i < N; ++i) {
    h1 += x[i] * w1[i];
    h2 += x[i] * w2[i];
    h3 += x[i] * w3[i];
  }
  // Computing non-linear activation function
  double h[3] = {h1, h2, h3};
  relu(h, 3);

  // Layer 2 - matrix multiplication
  double o = 0;
  for (int i = 0; i < 3; ++i)
    o += h[i] * w4[i];
  return o;
}

double loss_fn(double x[M * N], double y[M], double w1[N], double w2[N],
               double w3[N], double w4[3]) {
  double loss = 0;
  for (int i = 0; i < M; ++i) {
    double y_hat = model_fn(x + i * N, w1, w2, w3, w4);
    loss += (y_hat - y[i]) * (y_hat - y[i]);
  }
  return loss / M;
}

// Perform a single gradient descent step.
// theta_x are the hypothesis parameters, t is the generated dataset and
// clad_grad is the gradient function generated by Clad
template <typename T>
void performStep(double x[M * N], double y[M], double w1[N], double w2[N],
                 double w3[N], double w4[3], T clad_grad,
                 double learning_rate = 1e-2) {
  // Gradient computation
  double grad_w1[N], grad_w2[N], grad_w3[N], grad_w4[3];
  clad_grad.execute(x, y, w1, w2, w3, w4, grad_w1, grad_w2, grad_w3, grad_w4);

  // Update weights for hidden nodes
  for (int i = 0; i < N; ++i) {
    w1[i] -= learning_rate * grad_w1[i];
    w2[i] -= learning_rate * grad_w2[i];
    w3[i] -= learning_rate * grad_w3[i];
  }

  // Update weights for output node
  for (int i = 0; i < 3; ++i)
    w4[i] -= learning_rate * grad_w4[i];
}

// Perform gradient descent to optimize the weights.
void optimize(double x[M * N], double y[M], double w1[N], double w2[N],
              double w3[N], double w4[3], double lr, int num_iters) {
  auto grad = clad::gradient(loss_fn, "w1,w2,w3,w4");
  for (int i = 0; i < num_iters; ++i)
    performStep(x, y, w1, w2, w3, w4, grad, lr);
}

// Benchmark the time for 100 matrix multiplications.
void benchmark_matrix_multiply(double x[M + N], double y[M]) {
  double z[M * N];
  auto start_bench = std::chrono::high_resolution_clock::now();
  for (int i = 0; i < 100; ++i) {
    for (int j = 0; j < M; ++j) {
      for (int k = 0; k < N; ++k) {
        z[j * N + k] = 0;
        for (int l = 0; l < N; ++l)
          z[j * N + k] += x[j * N + l] * x[j * N + l];
      }
    }
  }
  auto end_bench = std::chrono::high_resolution_clock::now();
  std::chrono::duration<double> elapsed_bench = end_bench - start_bench;
  std::cout << "Time taken for 100 matrix multiplications: "
            << elapsed_bench.count() << " seconds" << std::endl;
}

int main() {
  // Initialize random number generator.
  std::mt19937 gen(42);

  // Generate random input data and labels.
  double x[M * N], y[M];
  for (int i = 0; i < M * N; ++i)
    x[i] = std::uniform_real_distribution<double>(0, 1)(gen);
  for (int i = 0; i < M; ++i)
    y[i] = std::uniform_real_distribution<double>(0, 1)(gen);

#ifdef BENCHMARK_MATRIX_MULTIPLY
  benchmark_matrix_multiply(x, y);
#endif

  // Initialize weights from a random distribution.
  double w1[N], w2[N], w3[N], w4[3];
  for (int i = 0; i < N; ++i) {
    w1[i] = std::uniform_real_distribution<double>(0, 1)(gen);
    w2[i] = std::uniform_real_distribution<double>(0, 1)(gen);
    w3[i] = std::uniform_real_distribution<double>(0, 1)(gen);
  }
  for (int i = 0; i < 3; ++i)
    w4[i] = std::uniform_real_distribution<double>(0, 1)(gen);

  // Calculate the loss before optimization.
  double loss = loss_fn(x, y, w1, w2, w3, w4);
  std::cout << "Initial loss before optimization: " << loss << std::endl;

  // Optimize the weights and compute the time taken.
  auto start = std::chrono::high_resolution_clock::now();
  optimize(x, y, w1, w2, w3, w4, 1e-2, N_ITER);
  auto end = std::chrono::high_resolution_clock::now();
  std::chrono::duration<double> elapsed = end - start;

  // Calculate the loss after optimization.
  loss = loss_fn(x, y, w1, w2, w3, w4);
  std::cout << "Final loss after optimization: " << loss << std::endl;
  std::cout << "Time taken: " << elapsed.count() << " seconds" << std::endl;
  return 0;
}
